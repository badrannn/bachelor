{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"colab":{"name":"Bachelor -  Fine-tune MARBERT.ipynb","provenance":[{"file_id":"1M0ls7EPUi1dwqIDh6HNfJ5y826XvcgGX","timestamp":1652740278853}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"JUiwkRv-XAkW"},"source":["# Download MARBERT checkpoint"]},{"cell_type":"code","metadata":{"id":"_8wBRd5zXAkX"},"source":["!wget https://huggingface.co/UBC-NLP/MARBERT/resolve/main/MARBERT_pytorch_verison.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9FYvgJevXAkY"},"source":["!tar -xvf MARBERT_pytorch_verison.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sUeiBs3tXAkZ"},"source":["!wget https://raw.githubusercontent.com/UBC-NLP/marbert/main/examples/UBC_AJGT_final_shuffled_train.tsv\n","!wget https://raw.githubusercontent.com/UBC-NLP/marbert/main/examples/UBC_AJGT_final_shuffled_test.tsv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yXM2ZsS1XAka","executionInfo":{"status":"ok","timestamp":1652741517244,"user_tz":-120,"elapsed":266,"user":{"displayName":"Abdelrahman Badran","userId":"00172406233395699283"}}},"source":["!mkdir -p AJGT\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"C2pkLcyfXAka"},"source":["!pip install GPUtil pytorch_pretrained_bert transformers pytorch-transformers pyarabic textblob tashaphyne"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6xBm7opGXAkb"},"source":["# Fine-tuning code"]},{"cell_type":"code","metadata":{"id":"9oOXwzeXXAkb"},"source":["# (1)load libraries \n","import json, sys, regex\n","import re\n","import torch\n","import GPUtil\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertAdam, BertForSequenceClassification\n","from tqdm import tqdm, trange\n","import pandas as pd\n","import os\n","import numpy as np\n","from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n","##----------------------------------------------------\n","from transformers import *\n","from transformers import XLMRobertaConfig\n","from transformers import XLMRobertaModel\n","from transformers import AutoTokenizer, AutoModelWithLMHead\n","from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, XLMRobertaModel\n","from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import AutoTokenizer, AutoModel\n","from textblob import TextBlob\n","from nltk.corpus import stopwords\n","from tashaphyne.stemming import ArabicLightStemmer\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GQWKMrXPXAkc"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print (\"your device \", device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KKujHwr5XAkd","executionInfo":{"status":"ok","timestamp":1652741304730,"user_tz":-120,"elapsed":273,"user":{"displayName":"Abdelrahman Badran","userId":"00172406233395699283"}}},"source":["\n","def create_label2ind_file(file, label_col):\n","\tlabels_json={}\n","\t#load train_dev_test file\n","\tdf = pd.read_csv(file, sep=\"\\t\")\n","\tdf.head(5)\n","\t#get labels and sort it A-Z\n","\tlabels = df[label_col].unique()\n","\tlabels.sort()\n","\t#convert labels to indexes\n","\tfor idx in range(0, len(labels)):\n","\t\tlabels_json[labels[idx]]=idx\n","\t#save labels with indexes to file\n","\twith open(label2idx_file, 'w') as json_file:\n","\t\tjson.dump(labels_json, json_file)\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"TNZxr8m6XAkd","executionInfo":{"status":"ok","timestamp":1652756187467,"user_tz":-120,"elapsed":252,"user":{"displayName":"Abdelrahman Badran","userId":"00172406233395699283"}}},"source":["\n","def data_prepare_BERT(file_path, lab2ind, tokenizer, content_col, label_col, MAX_LEN):\n","\t# Use pandas to load dataset\n","\tdf = pd.read_csv(file_path, delimiter='\\t', header=0)\n","\tdf = df[df[content_col].notnull()]\n","\tdf = df[df[label_col].notnull()]\n","\tprint(\"Data size \", df.shape)\n","\t# Create sentence and label lists\n","\tsentences = df[content_col].values\n","\tsentences = [\"[CLS] \" + data_cleaning(sentence) + \" [SEP]\" for sentence in sentences] # Run processing function\n","\tprint (\"The first sentence:\")\n","\tprint (sentences[0])\n","\t# Create sentence and label lists\n","\tlabels = df[label_col].values\n","\t#print (labels)\n","\tlabels = [lab2ind[i] for i in labels]\n","\t# Import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary.\n","\ttokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\tprint (\"Tokenize the first sentence:\")\n","\tprint (tokenized_texts[0])\n","\t#print(\"Label is \", labels[0])\n","\t# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n","\tinput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\tprint (\"Index numbers of the first sentence:\")\n","\tprint (input_ids[0])\n","\t# Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token\n","\t# ~ input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\tpad_ind = tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n","\tinput_ids = pad_sequences(input_ids, maxlen=MAX_LEN+2, dtype=\"long\", truncating=\"post\", padding=\"post\", value=pad_ind)\n","\tprint (\"Index numbers of the first sentence after padding:\\n\",input_ids[0])\n","\t# Create attention masks\n","\tattention_masks = []\n","\t# Create a mask of 1s for each token followed by 0s for padding\n","\tfor seq in input_ids:\n","\t\tseq_mask = [float(i > 0) for i in seq]\n","\t\tattention_masks.append(seq_mask)\n","\t# Convert all of our data into torch tensors, the required datatype for our model\n","\tinputs = torch.tensor(input_ids)\n","\tlabels = torch.tensor(labels)\n","\tmasks = torch.tensor(attention_masks)\n","\treturn inputs, labels, masks\n"],"execution_count":84,"outputs":[]},{"cell_type":"code","source":["# data cleaning function\n","stops = set(stopwords.words(\"arabic\"))\n","stop_word_comp = {\"،\",\"آض\",\"آمينَ\",\"آه\",\"آهاً\",\"آي\",\"أ\",\"أب\",\"أجل\",\"أجمع\",\"أخ\",\"أخذ\",\"أصبح\",\"أضحى\",\"أقبل\",\"أقل\",\"أكثر\",\"ألا\",\"أم\",\"أما\",\"أمامك\",\"أمامكَ\",\"أمسى\",\"أمّا\",\"أن\",\"أنا\",\"أنت\",\"أنتم\",\"أنتما\",\"أنتن\",\"أنتِ\",\"أنشأ\",\"أنّى\",\"أو\",\"أوشك\",\"أولئك\",\"أولئكم\",\"أولاء\",\"أولالك\",\"أوّهْ\",\"أي\",\"أيا\",\"أين\",\"أينما\",\"أيّ\",\"أَنَّ\",\"أََيُّ\",\"أُفٍّ\",\"إذ\",\"إذا\",\"إذاً\",\"إذما\",\"إذن\",\"إلى\",\"إليكم\",\"إليكما\",\"إليكنّ\",\"إليكَ\",\"إلَيْكَ\",\"إلّا\",\"إمّا\",\"إن\",\"إنّما\",\"إي\",\"إياك\",\"إياكم\",\"إياكما\",\"إياكن\",\"إيانا\",\"إياه\",\"إياها\",\"إياهم\",\"إياهما\",\"إياهن\",\"إياي\",\"إيهٍ\",\"إِنَّ\",\"ا\",\"ابتدأ\",\"اثر\",\"اجل\",\"احد\",\"اخرى\",\"اخلولق\",\"اذا\",\"اربعة\",\"ارتدّ\",\"استحال\",\"اطار\",\"اعادة\",\"اعلنت\",\"اف\",\"اكثر\",\"اكد\",\"الألاء\",\"الألى\",\"الا\",\"الاخيرة\",\"الان\",\"الاول\",\"الاولى\",\"التى\",\"التي\",\"الثاني\",\"الثانية\",\"الذاتي\",\"الذى\",\"الذي\",\"الذين\",\"السابق\",\"الف\",\"اللائي\",\"اللاتي\",\"اللتان\",\"اللتيا\",\"اللتين\",\"اللذان\",\"اللذين\",\"اللواتي\",\"الماضي\",\"المقبل\",\"الوقت\",\"الى\",\"اليوم\",\"اما\",\"امام\",\"امس\",\"ان\",\"انبرى\",\"انقلب\",\"انه\",\"انها\",\"او\",\"اول\",\"اي\",\"ايار\",\"ايام\",\"ايضا\",\"ب\",\"بات\",\"باسم\",\"بان\",\"بخٍ\",\"برس\",\"بسبب\",\"بسّ\",\"بشكل\",\"بضع\",\"بطآن\",\"بعد\",\"بعض\",\"بك\",\"بكم\",\"بكما\",\"بكن\",\"بل\",\"بلى\",\"بما\",\"بماذا\",\"بمن\",\"بن\",\"بنا\",\"به\",\"بها\",\"بي\",\"بيد\",\"بين\",\"بَسْ\",\"بَلْهَ\",\"بِئْسَ\",\"تانِ\",\"تانِك\",\"تبدّل\",\"تجاه\",\"تحوّل\",\"تلقاء\",\"تلك\",\"تلكم\",\"تلكما\",\"تم\",\"تينك\",\"تَيْنِ\",\"تِه\",\"تِي\",\"ثلاثة\",\"ثم\",\"ثمّ\",\"ثمّة\",\"ثُمَّ\",\"جعل\",\"جلل\",\"جميع\",\"جير\",\"حار\",\"حاشا\",\"حاليا\",\"حاي\",\"حتى\",\"حرى\",\"حسب\",\"حم\",\"حوالى\",\"حول\",\"حيث\",\"حيثما\",\"حين\",\"حيَّ\",\"حَبَّذَا\",\"حَتَّى\",\"حَذارِ\",\"خلا\",\"خلال\",\"دون\",\"دونك\",\"ذا\",\"ذات\",\"ذاك\",\"ذانك\",\"ذانِ\",\"ذلك\",\"ذلكم\",\"ذلكما\",\"ذلكن\",\"ذو\",\"ذوا\",\"ذواتا\",\"ذواتي\",\"ذيت\",\"ذينك\",\"ذَيْنِ\",\"ذِه\",\"ذِي\",\"راح\",\"رجع\",\"رويدك\",\"ريث\",\"رُبَّ\",\"زيارة\",\"سبحان\",\"سرعان\",\"سنة\",\"سنوات\",\"سوف\",\"سوى\",\"سَاءَ\",\"سَاءَمَا\",\"شبه\",\"شخصا\",\"شرع\",\"شَتَّانَ\",\"صار\",\"صباح\",\"صفر\",\"صهٍ\",\"صهْ\",\"ضد\",\"ضمن\",\"طاق\",\"طالما\",\"طفق\",\"طَق\",\"ظلّ\",\"عاد\",\"عام\",\"عاما\",\"عامة\",\"عدا\",\"عدة\",\"عدد\",\"عدم\",\"عسى\",\"عشر\",\"عشرة\",\"علق\",\"على\",\"عليك\",\"عليه\",\"عليها\",\"علًّ\",\"عن\",\"عند\",\"عندما\",\"عوض\",\"عين\",\"عَدَسْ\",\"عَمَّا\",\"غدا\",\"غير\",\"ـ\",\"ف\",\"فان\",\"فلان\",\"فو\",\"فى\",\"في\",\"فيم\",\"فيما\",\"فيه\",\"فيها\",\"قال\",\"قام\",\"قبل\",\"قد\",\"قطّ\",\"قلما\",\"قوة\",\"كأنّما\",\"كأين\",\"كأيّ\",\"كأيّن\",\"كاد\",\"كان\",\"كانت\",\"كذا\",\"كذلك\",\"كرب\",\"كل\",\"كلا\",\"كلاهما\",\"كلتا\",\"كلم\",\"كليكما\",\"كليهما\",\"كلّما\",\"كلَّا\",\"كم\",\"كما\",\"كي\",\"كيت\",\"كيف\",\"كيفما\",\"كَأَنَّ\",\"كِخ\",\"لئن\",\"لا\",\"لات\",\"لاسيما\",\"لدن\",\"لدى\",\"لعمر\",\"لقاء\",\"لك\",\"لكم\",\"لكما\",\"لكن\",\"لكنَّما\",\"لكي\",\"لكيلا\",\"للامم\",\"لم\",\"لما\",\"لمّا\",\"لن\",\"لنا\",\"له\",\"لها\",\"لو\",\"لوكالة\",\"لولا\",\"لوما\",\"لي\",\"لَسْتَ\",\"لَسْتُ\",\"لَسْتُم\",\"لَسْتُمَا\",\"لَسْتُنَّ\",\"لَسْتِ\",\"لَسْنَ\",\"لَعَلَّ\",\"لَكِنَّ\",\"لَيْتَ\",\"لَيْسَ\",\"لَيْسَا\",\"لَيْسَتَا\",\"لَيْسَتْ\",\"لَيْسُوا\",\"لَِسْنَا\",\"ما\",\"ماانفك\",\"مابرح\",\"مادام\",\"ماذا\",\"مازال\",\"مافتئ\",\"مايو\",\"متى\",\"مثل\",\"مذ\",\"مساء\",\"مع\",\"معاذ\",\"مقابل\",\"مكانكم\",\"مكانكما\",\"مكانكنّ\",\"مكانَك\",\"مليار\",\"مليون\",\"مما\",\"ممن\",\"من\",\"منذ\",\"منها\",\"مه\",\"مهما\",\"مَنْ\",\"مِن\",\"نحن\",\"نحو\",\"نعم\",\"نفس\",\"نفسه\",\"نهاية\",\"نَخْ\",\"نِعِمّا\",\"نِعْمَ\",\"ها\",\"هاؤم\",\"هاكَ\",\"هاهنا\",\"هبّ\",\"هذا\",\"هذه\",\"هكذا\",\"هل\",\"هلمَّ\",\"هلّا\",\"هم\",\"هما\",\"هن\",\"هنا\",\"هناك\",\"هنالك\",\"هو\",\"هي\",\"هيا\",\"هيت\",\"هيّا\",\"هَؤلاء\",\"هَاتانِ\",\"هَاتَيْنِ\",\"هَاتِه\",\"هَاتِي\",\"هَجْ\",\"هَذا\",\"هَذانِ\",\"هَذَيْنِ\",\"هَذِه\",\"هَذِي\",\"هَيْهَاتَ\",\"و\",\"و6\",\"وا\",\"واحد\",\"واضاف\",\"واضافت\",\"واكد\",\"وان\",\"واهاً\",\"واوضح\",\"وراءَك\",\"وفي\",\"وقال\",\"وقالت\",\"وقد\",\"وقف\",\"وكان\",\"وكانت\",\"ولا\",\"ولم\",\"ومن\",\"مَن\",\"وهو\",\"وهي\",\"ويكأنّ\",\"وَيْ\",\"وُشْكَانََ\",\"يكون\",\"يمكن\",\"يوم\",\"ّأيّان\"}\n","ArListem = ArabicLightStemmer()\n","\n","\n","def data_cleaning(sentence):\n","  # ------------ Step 1: remove non arabic characters ------------\n","  output = re.sub(r'\\s*[A-Za-z]+\\b', '' , sentence)\n","  output = output.rstrip()\n","  \n","  # ------------ Step 2: normalize arabic ------------\n","  normalized = normalize_arabic(output)\n","\n","  # ------------ Step 3: stop words removal ------------\n","  removed = remove_stop_words(normalized)\n","\n","  # ------------ Step 4: tweet specific cleaning ------------\n","  tweet_cleaned = clean_tweet(removed)\n","\n","  return tweet_cleaned\n","\n","def clean_tweet(text):\n","    text = re.sub('#\\d+K\\d+', ' ', text)  # years like 2K19\n","    text = re.sub('http\\S+\\s*', ' ', text)  # remove URLs\n","    text = re.sub('RT|cc', ' ', text)  # remove RT and cc\n","    text = re.sub('@[^\\s]+',' ',text)\n","    text = clean_hashtag(text)\n","    text = clean_emoji(text)\n","    return text\n","\n","def clean_emoji(text):\n","  emoji_pattern = re.compile(\"[\"\n","                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                                   u\"\\U00002702-\\U000027B0\"\n","                                   u\"\\U000024C2-\\U0001F251\"\n","                                   \"]+\", flags=re.UNICODE)\n","  text = emoji_pattern.sub(r'', text)\n","  return text\n","\n","def clean_hashtag(text):\n","  words = text.split()\n","  text = list()\n","  for word in words:\n","    if is_hashtag(word):\n","      text.extend(extract_hashtag(word))\n","    else:\n","      text.append(word)\n","    return \" \".join(text)\n","\n","def is_hashtag(word):\n","    if word.startswith(\"#\"):\n","        return True\n","    else:\n","        return False\n","\n","def extract_hashtag(text):\n","    hash_list = ([re.sub(r\"(\\W+)$\", \"\", i) for i in text.split() if i.startswith(\"#\")])\n","    word_list = []\n","    for word in hash_list :\n","        word_list.extend(split_hashtag_to_words(word))\n","    return word_list\n","\n","def split_hashtag_to_words(tag):\n","    tag = tag.replace('#','')\n","    tags = tag.split('_')\n","    if len(tags) > 1 :\n","        \n","        return tags\n","    pattern = re.compile(r\"[A-Z][a-z]+|\\d+|[A-Z]+(?![a-z])\")\n","    return pattern.findall(tag)\n","\n","\n","\n","\n","def normalize_arabic(text):\n","  import pyarabic.araby as araby\n","  text = text.strip()\n","  text = re.sub(\"[إأٱآا]\", \"ا\", text)\n","  text = re.sub(\"ى\", \"ي\", text)\n","  text = re.sub(\"ؤ\", \"ء\", text)\n","  text = re.sub(\"ئ\", \"ء\", text)\n","  text = re.sub(\"ة\", \"ه\", text)\n","  noise = re.compile(\"\"\" ّ    | # Tashdid\n","                             َ    | # Fatha\n","                             ً    | # Tanwin Fath\n","                             ُ    | # Damma\n","                             ٌ    | # Tanwin Damm\n","                             ِ    | # Kasra\n","                             ٍ    | # Tanwin Kasr\n","                             ْ    | # Sukun\n","                             ـ     # Tatwil/Kashida\n","                         \"\"\", re.VERBOSE)\n","  text = re.sub(noise, '', text)\n","  text = re.sub(r'(.)\\1+', r\"\\1\\1\", text) # Remove longation\n","  return araby.strip_tashkeel(text)\n","  \n","def remove_stop_words(text):\n","    zen = TextBlob(text)\n","    words = zen.words\n","    return \" \".join([w for w in words if not w in stops and not w in stop_word_comp and len(w) >= 2])\n","\n"],"metadata":{"id":"1L33sdyhTEby","executionInfo":{"status":"ok","timestamp":1652756185028,"user_tz":-120,"elapsed":271,"user":{"displayName":"Abdelrahman Badran","userId":"00172406233395699283"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","metadata":{"id":"gdPCPv8VXAke","executionInfo":{"status":"ok","timestamp":1652743128938,"user_tz":-120,"elapsed":269,"user":{"displayName":"Abdelrahman Badran","userId":"00172406233395699283"}}},"source":["# Function to calculate the accuracy of our predictions vs labels\n","# def flat_accuracy(preds, labels):\n","#\t  pred_flat = np.argmax(preds, axis=1).flatten()\n","#\t  labels_flat = labels.flatten()\n","#\t  return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","def flat_pred(preds, labels):\n","\tpred_flat = np.argmax(preds, axis=1).flatten()\n","\tlabels_flat = labels.flatten()\n","\treturn pred_flat.tolist(), labels_flat.tolist()"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"5vyvc8JoXAke","executionInfo":{"status":"ok","timestamp":1652743132539,"user_tz":-120,"elapsed":259,"user":{"displayName":"Abdelrahman Badran","userId":"00172406233395699283"}}},"source":["\n","def train(model, iterator, optimizer, scheduler, criterion):\n","\t\n","\tmodel.train()\n","\tepoch_loss = 0\n","\tfor i, batch in enumerate(iterator):\n","\t\t# Add batch to GPU\n","\t\tbatch = tuple(t.to(device) for t in batch)\n","\t\t# Unpack the inputs from our dataloader\n","\t\tinput_ids, input_mask, labels = batch\n","\t\toutputs = model(input_ids, input_mask, labels=labels)\n","\t\tloss, logits = outputs[:2]\n","\t\t# delete used variables to free GPU memory\n","\t\tdel batch, input_ids, input_mask, labels\n","\t\toptimizer.zero_grad()\n","\t\tif torch.cuda.device_count() == 1:\n","\t\t\tloss.backward()\n","\t\t\tepoch_loss += loss.cpu().item()\n","\t\telse:\n","\t\t\tloss.sum().backward()\n","\t\t\tepoch_loss += loss.sum().cpu().item()\n","\t\toptimizer.step()\n","\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore\n","\t\t# optimizer.step()\n","\t\tscheduler.step()\n","\t# free GPU memory\n","\tif device == 'cuda':\n","\t\ttorch.cuda.empty_cache()\n","\treturn epoch_loss / len(iterator)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hjwa6a3bXAkf","executionInfo":{"status":"ok","timestamp":1652743136754,"user_tz":-120,"elapsed":280,"user":{"displayName":"Abdelrahman Badran","userId":"00172406233395699283"}}},"source":["\n","def evaluate(model, iterator, criterion):\n","\tmodel.eval()\n","\tepoch_loss = 0\n","\tall_pred=[]\n","\tall_label = []\n","\twith torch.no_grad():\n","\t\tfor i, batch in enumerate(iterator):\n","\t\t\t# Add batch to GPU\n","\t\t\tbatch = tuple(t.to(device) for t in batch)\n","\t\t\t# Unpack the inputs from our dataloader\n","\t\t\tinput_ids, input_mask, labels = batch\n","\t\t\toutputs = model(input_ids, input_mask, labels=labels)\n","\t\t\tloss, logits = outputs[:2]\n","\t\t\t# delete used variables to free GPU memory\n","\t\t\tdel batch, input_ids, input_mask\n","\t\t\tif torch.cuda.device_count() == 1:\n","\t\t\t\tepoch_loss += loss.cpu().item()\n","\t\t\telse:\n","\t\t\t\tepoch_loss += loss.sum().cpu().item()\n","\t\t\t# identify the predicted class for each example in the batch\n","\t\t\tprobabilities, predicted = torch.max(logits.cpu().data, 1)\n","\t\t\t# put all the true labels and predictions to two lists\n","\t\t\tall_pred.extend(predicted)\n","\t\t\tall_label.extend(labels.cpu())\n","\taccuracy = accuracy_score(all_label, all_pred)\n","\tf1score = f1_score(all_label, all_pred, average='macro') \n","\trecall = recall_score(all_label, all_pred, average='macro')\n","\tprecision = precision_score(all_label, all_pred, average='macro')\n","\treport = classification_report(all_label, all_pred)\n","\treturn (epoch_loss / len(iterator)), accuracy, f1score, recall, precision\n","\n"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"5nWmg6P-XAkf","executionInfo":{"status":"ok","timestamp":1652749750777,"user_tz":-120,"elapsed":259,"user":{"displayName":"Abdelrahman Badran","userId":"00172406233395699283"}}},"source":["\n","def fine_tuning(config):\n","\t#---------------------------------------\n","\tprint (\"[INFO] step (1) load train_test config file\")\n","\t# config_file = open(config_file, 'r', encoding=\"utf8\")\n","\t# config = json.load(config_file)\n","\ttask_name = config[\"task_name\"]\n","\tcontent_col = config[\"content_col\"]\n","\tlabel_col = config[\"label_col\"]\n","\ttrain_file = config[\"data_dir\"]+config[\"train_file\"]\n","\tdev_file = config[\"data_dir\"]+config[\"dev_file\"]\n","\tsortby = config[\"sortby\"]\n","\tmax_seq_length= int(config[\"max_seq_length\"])\n","\tbatch_size = int(config[\"batch_size\"])\n","\tlr_var = float(config[\"lr\"])\n","\tmodel_path = config['pretrained_model_path']\n","\tnum_epochs = config['epochs'] # Number of training epochs (authors recommend between 2 and 4)\n","\tglobal label2idx_file\n","\tlabel2idx_file = config[\"data_dir\"]+config[\"task_name\"]+\"_labels-dict.json\"\n","\t#-------------------------------------------------------\n","\tprint (\"[INFO] step (2) convert labels2index\")\n","\tcreate_label2ind_file(train_file, label_col)\n","\tprint (label2idx_file)\n","\t#---------------------------------------------------------\n","\tprint (\"[INFO] step (3) check checkpoit directory and report file\")\n","\tckpt_dir = config[\"data_dir\"]+task_name+\"_bert_ckpt/\"\n","\treport = ckpt_dir+task_name+\"_report.tsv\"\n","\tsorted_report = ckpt_dir+task_name+\"_report_sorted.tsv\"\n","\tif not os.path.exists(ckpt_dir):\n","\t\tos.mkdir(ckpt_dir)\n","\t#-------------------------------------------------------\n","\tprint (\"[INFO] step (4) load label to number dictionary\")\n","\tlab2ind = json.load(open(label2idx_file))\n","\tprint (\"[INFO] train_file\", train_file)\n","\tprint (\"[INFO] dev_file\", dev_file)\n","\tprint (\"[INFO] num_epochs\", num_epochs)\n","\tprint (\"[INFO] model_path\", model_path)\n","\tprint (\"max_seq_length\", max_seq_length, \"batch_size\", batch_size)\n","\t#-------------------------------------------------------\n","\tprint (\"[INFO] step (5) Use defined funtion to extract tokanize data\")\n","\t# tokenizer from pre-trained BERT model\n","\tprint (\"loading BERT setting\")\n","\ttokenizer = BertTokenizer.from_pretrained(model_path)\n","\ttrain_inputs, train_labels, train_masks = data_prepare_BERT(train_file, lab2ind, tokenizer,content_col, label_col, max_seq_length)\n","\tvalidation_inputs, validation_labels, validation_masks = data_prepare_BERT(dev_file, lab2ind, tokenizer, content_col, label_col,max_seq_length)\n","\t# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n","\tmodel = BertForSequenceClassification.from_pretrained(model_path, num_labels=len(lab2ind))\n","\t#--------------------------------------\n","\tprint (\"[INFO] step (6) Create an iterator of data with torch DataLoader.\")\n","#\t\t  This helps save on memory during training because, unlike a for loop,\\\n","#\t\t  with an iterator the entire dataset does not need to be loaded into memory\")\n","\ttrain_data = TensorDataset(train_inputs, train_masks, train_labels)\n","\ttrain_dataloader = DataLoader(train_data, batch_size=batch_size)\n","\t#---------------------------\n","\tvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","\tvalidation_dataloader = DataLoader(validation_data, batch_size=batch_size)\n","\t#------------------------------------------\n","\tprint (\"[INFO] step (7) run with parallel GPUs\")\n","\tif torch.cuda.is_available():\n","\t\tif torch.cuda.device_count() == 1:\n","\t\t\tprint(\"Run\", \"with one GPU\")\n","\t\t\tmodel = model.to(device)\n","\t\telse:\n","\t\t\tn_gpu = torch.cuda.device_count()\n","\t\t\tprint(\"Run\", \"with\", n_gpu, \"GPUs with max 4 GPUs\")\n","\t\t\tdevice_ids = GPUtil.getAvailable(limit = 4)\n","\t\t\ttorch.backends.cudnn.benchmark = True\n","\t\t\tmodel = model.to(device)\n","\t\t\tmodel = nn.DataParallel(model, device_ids=device_ids)\n","\telse:\n","\t\tprint(\"Run\", \"with CPU\")\n","\t\tmodel = model\n","\t#---------------------------------------------------\n","\tprint (\"[INFO] step (8) set Parameters, schedules, and loss function\")\n","\tglobal max_grad_norm\n","\tmax_grad_norm = 1.0\n","\twarmup_proportion = 0.1\n","\tnum_training_steps\t= len(train_dataloader) * num_epochs\n","\tnum_warmup_steps = num_training_steps * warmup_proportion\n","\t### In Transformers, optimizer and schedules are instantiated like this:\n","\t# Note: AdamW is a class from the huggingface library\n","\t# the 'W' stands for 'Weight Decay\"\n","\toptimizer = AdamW(model.parameters(), lr=lr_var, correct_bias=False)\n","\t# schedules\n","\tscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n","\t# We use nn.CrossEntropyLoss() as our loss function. \n","\tcriterion = nn.CrossEntropyLoss()\n","\t#---------------------------------------------------\n","\tprint (\"[INFO] step (9) start fine_tuning\")\n","\tfor epoch in trange(num_epochs, desc=\"Epoch\"):\n","\t\ttrain_loss = train(model, train_dataloader, optimizer, scheduler, criterion)\t  \n","\t\tval_loss, val_acc, val_f1, val_recall, val_precision = evaluate(model, validation_dataloader, criterion)\n","# \t\tprint (train_loss, val_acc)\n","\t\t# Create checkpoint at end of each epoch\n","\t\tif not os.path.exists(ckpt_dir + 'model_' + str(int(epoch + 1)) + '/'): os.mkdir(ckpt_dir + 'model_' + str(int(epoch + 1)) + '/')\n","\t\tmodel.save_pretrained(ckpt_dir+ 'model_' + str(int(epoch + 1)) + '/')\n","\t\tepoch_eval_results = {\"epoch_num\":int(epoch + 1),\"train_loss\":train_loss,\n","\t\t\t\t\t  \"val_acc\":val_acc, \"val_recall\":val_recall, \"val_precision\":val_precision, \"val_f1\":val_f1,\"lr\":lr_var }\n","\t\twith open(report,\"a\") as fOut:\n","\t\t\tfOut.write(json.dumps(epoch_eval_results)+\"\\n\")\n","\t\t\tfOut.flush()\n","\t\t#------------------------------------\n","\t\treport_df = pd.read_json(report, orient='records', lines=True)\n","\t\treport_df.sort_values(by=[sortby],ascending=False, inplace=True)\n","\t\treport_df.to_csv(sorted_report,sep=\"\\t\",index=False)\n","\treturn report_df"],"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xDnDyOFHiZvk"},"source":["# Run fine-tuning for 5 epochs"]},{"cell_type":"code","metadata":{"id":"Ol6LaSAAXAki","executionInfo":{"status":"ok","timestamp":1652749747976,"user_tz":-120,"elapsed":464,"user":{"displayName":"Abdelrahman Badran","userId":"00172406233395699283"}}},"source":["\n","config={\"task_name\": \"AJGT_MARBERT\", #output directory name\n","             \"data_dir\": \"./AJGT/\", #data directory\n","             \"train_file\": \"DA_train_labeled.tsv\", #train file path\n","             \"dev_file\": \"DA_dev_labeled.tsv\", #dev file path or test file path\n","             \"pretrained_model_path\": 'MARBERT_pytorch_verison', #MARBERT checkpoint path\n","             \"epochs\": 1, #number of epochs\n","             \"content_col\": \"#2_tweet\", #text column\n","             \"label_col\": \"#3_country_label\", #label column\n","             \"lr\": 2e-06, #learning rate\n","              \"max_seq_length\": 128, #max sequance length\n","              \"batch_size\": 16, #batch shize\n","              \"sortby\":\"val_acc\"} #sort results based on val_acc or val_f1\n"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"jq9kOFoaXAkj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652750535032,"user_tz":-120,"elapsed":574196,"user":{"displayName":"Abdelrahman Badran","userId":"00172406233395699283"}},"outputId":"2b8f2414-b1c8-49cf-adec-c6ddc89c1c00"},"source":["report_df = fine_tuning(config)"],"execution_count":42,"outputs":[{"output_type":"stream","name":"stderr","text":["Didn't find file MARBERT_pytorch_verison/added_tokens.json. We won't load it.\n","Didn't find file MARBERT_pytorch_verison/special_tokens_map.json. We won't load it.\n","Didn't find file MARBERT_pytorch_verison/tokenizer_config.json. We won't load it.\n","loading file MARBERT_pytorch_verison/vocab.txt\n","loading file None\n","loading file None\n","loading file None\n","loading configuration file MARBERT_pytorch_verison/config.json\n"]},{"output_type":"stream","name":"stdout","text":["[INFO] step (1) load train_test config file\n","[INFO] step (2) convert labels2index\n","./AJGT/AJGT_MARBERT_labels-dict.json\n","[INFO] step (3) check checkpoit directory and report file\n","[INFO] step (4) load label to number dictionary\n","[INFO] train_file ./AJGT/DA_train_labeled.tsv\n","[INFO] dev_file ./AJGT/DA_dev_labeled.tsv\n","[INFO] num_epochs 1\n","[INFO] model_path MARBERT_pytorch_verison\n","max_seq_length 128 batch_size 16\n","[INFO] step (5) Use defined funtion to extract tokanize data\n","loading BERT setting\n","Data size  (21000, 4)\n","The first sentence:\n","[CLS] حاجة حلوة اكيد [SEP]\n","Tokenize the first sentence:\n","['[CLS]', 'حاجة', 'حلوة', 'اكيد', '[SEP]']\n","Index numbers of the first sentence:\n","[2, 2827, 4650, 4151, 3]\n","Index numbers of the first sentence after padding:\n"," [   2 2827 4650 4151    3    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0]\n","Data size  (5000, 4)\n","The first sentence:\n","[CLS] قولنا اون لاين لا يا علي اون لاين لا [SEP]\n","Tokenize the first sentence:\n","['[CLS]', 'قولنا', 'اون', 'لاين', 'لا', 'يا', 'علي', 'اون', 'لاين', 'لا', '[SEP]']\n","Index numbers of the first sentence:\n","[2, 26284, 8928, 6992, 1956, 2023, 1998, 8928, 6992, 1956, 3]\n","Index numbers of the first sentence after padding:\n"," [    2 26284  8928  6992  1956  2023  1998  8928  6992  1956     3     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0]\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file MARBERT_pytorch_verison/config.json\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 100000\n","}\n","\n","loading weights file MARBERT_pytorch_verison/pytorch_model.bin\n","Some weights of the model checkpoint at MARBERT_pytorch_verison were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at MARBERT_pytorch_verison and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["[INFO] step (6) Create an iterator of data with torch DataLoader.\n","[INFO] step (7) run with parallel GPUs\n","Run with one GPU\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["[INFO] step (8) set Parameters, schedules, and loss function\n","[INFO] step (9) start fine_tuning\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:   0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Configuration saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_1/config.json\n","Model weights saved in ./AJGT/AJGT_MARBERT_bert_ckpt/model_1/pytorch_model.bin\n","Epoch: 100%|██████████| 1/1 [09:21<00:00, 561.09s/it]\n"]}]},{"cell_type":"code","metadata":{"id":"5V9ZOdVNXAkk","colab":{"base_uri":"https://localhost:8080/","height":175},"executionInfo":{"status":"ok","timestamp":1652750640165,"user_tz":-120,"elapsed":286,"user":{"displayName":"Abdelrahman Badran","userId":"00172406233395699283"}},"outputId":"6c0b947b-5c67-46c3-de4e-6c4b8597fb9e"},"source":["report_df.head(5)"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   epoch_num  train_loss  val_acc  val_recall  val_precision    val_f1  \\\n","1          2    1.747341   0.4958    0.240542       0.209736  0.210788   \n","2          1    2.082762   0.4906    0.224540       0.204548  0.198038   \n","0          1    2.115478   0.4886    0.220809       0.208834  0.196122   \n","\n","         lr  \n","1  0.000002  \n","2  0.000002  \n","0  0.000002  "],"text/html":["\n","  <div id=\"df-96ddcb1a-962a-467d-887e-d2ad040df927\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>epoch_num</th>\n","      <th>train_loss</th>\n","      <th>val_acc</th>\n","      <th>val_recall</th>\n","      <th>val_precision</th>\n","      <th>val_f1</th>\n","      <th>lr</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1.747341</td>\n","      <td>0.4958</td>\n","      <td>0.240542</td>\n","      <td>0.209736</td>\n","      <td>0.210788</td>\n","      <td>0.000002</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>2.082762</td>\n","      <td>0.4906</td>\n","      <td>0.224540</td>\n","      <td>0.204548</td>\n","      <td>0.198038</td>\n","      <td>0.000002</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>2.115478</td>\n","      <td>0.4886</td>\n","      <td>0.220809</td>\n","      <td>0.208834</td>\n","      <td>0.196122</td>\n","      <td>0.000002</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-96ddcb1a-962a-467d-887e-d2ad040df927')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-96ddcb1a-962a-467d-887e-d2ad040df927 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-96ddcb1a-962a-467d-887e-d2ad040df927');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":43}]}]}